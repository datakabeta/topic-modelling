{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzman Machine \n",
    "RBMs are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input, layer, and the second is the hidden layer. The absense of an output layer is apparent. As we move forward we would learn why the output layer won't be needed.\n",
    "<img src=\"figure1.png\" width=\"150\" height=\"50\" title=\"Layers in RBM\">\n",
    "Figure1: Layers in RBM\n",
    "Each circle in the figure above represents a neuron-like unit called a node, and nodes are simply where calculations take place. \n",
    "<img src=\"figure3.png\" width=\"500\" height=\"200\" title=\"Layers in RBM\">\n",
    "Figure2: Structure of RBM\n",
    "The nodes are connected to each other across layers, but no two nodes of the same layer are linked. That is, there is no intra-layer communication â€“ this is the restriction in a restricted Boltzmann machine. Each node is a locus of computation that processes input, and begins by making stochastic decisions about whether to transmit that input or not. Each visible node takes a low-level feature from an item in the dataset to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages that will be required in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np_rng = np.random.RandomState(1234) #setting the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show all ouput\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graded\n",
    "# import data\n",
    "df = pd.read_excel('amazon.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  So there is no way for me to plug it in here i...\n",
       "1                        Good case, Excellent value.\n",
       "2                             Great for the jawbone.\n",
       "3  Tied to charger for conversations lasting more...\n",
       "4                                  The mic is great."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this and check if you have got the correct output\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Topic Modelling, you find the best set of topics that describe the document. There are various ways to perform topic modelling one of which is RBM. You train your RBM on a set of documents. \n",
    "The visible layers will be the words in the text, the hidden layers will give the Topics. \n",
    "To input words into the visible layer, let's convert the trai data above into a bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1642)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1642"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#graded\n",
    "# create bag of words model \n",
    "#the final shape should be (number of documents, vocabulary)\n",
    "# fit tf on the dataframe df\n",
    "tf = CountVectorizer(stop_words='english',max_df=0.5) \n",
    "trainX = tf.fit_transform(df.Text)\n",
    "\n",
    "trainX.shape\n",
    "len(tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: There are 1642 words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>15g</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>2000</th>\n",
       "      <th>...</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "      <th>z500a</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1642 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  100  11  12  13  15  15g  18  20  2000  ...  wouldn  wow  wrong  \\\n",
       "0   0    0   0   0   0   0    0   0   0     0  ...       0    0      0   \n",
       "1   0    0   0   0   0   0    0   0   0     0  ...       0    0      0   \n",
       "2   0    0   0   0   0   0    0   0   0     0  ...       0    0      0   \n",
       "3   0    0   0   0   0   0    0   0   0     0  ...       0    0      0   \n",
       "4   0    0   0   0   0   0    0   0   0     0  ...       0    0      0   \n",
       "\n",
       "   wrongly  year  years  yell  yes  z500a  zero  \n",
       "0        0     0      0     0    0      0     0  \n",
       "1        0     0      0     0    0      0     0  \n",
       "2        0     0      0     0    0      0     0  \n",
       "3        0     0      0     0    0      0     0  \n",
       "4        0     0      0     0    0      0     0  \n",
       "\n",
       "[5 rows x 1642 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the dataframe\n",
    "pd.DataFrame(trainX.toarray(), columns = tf.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the vectorizer worked well taking 1st review as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there is no way for me to plug it in here in the US unless I go by a converter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 326, 1081, 1524, 1582], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[0].Text) #1st review\n",
    "np.where(trainX.toarray()[0]==1)[0] #get indices in the vectorized review where the value is 1 i.e. word exists in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'converter'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'plug'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'unless'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'way'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1642)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get words in vocabulary with same indices\n",
    "tf.get_feature_names()[326]\n",
    "tf.get_feature_names()[1081]\n",
    "tf.get_feature_names()[1524]\n",
    "tf.get_feature_names()[1582]\n",
    "\n",
    "#check how many words are there in vocabulary for 1st sentence\n",
    "print(sum(trainX.toarray()[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: In the bag-of-words vectorized data, the 1st review (document) has the words 'converter','plug','unless','way' set to 1 which is as expected as these words are part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the bag of words model, let's define the number of visible and hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# define visible units\n",
    "visibleUnits = trainX.shape[1] # vocabulary size ~1 line\n",
    "\n",
    "# assign number of units\n",
    "hiddenUnits = 5 # hyperparameter, this means that we are looking for 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1642"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visibleUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# utility Functions\n",
    "\n",
    "# deine the sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X)) # ~ 1 line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM as a Probabilistic Model\n",
    "Restricted Boltzmann Machines are probabilistic. As opposed to assigning discrete values the model assigns probabilities. At each point in time the RBM is in a certain state. The state refers to the values of neurons in the visible and hidden layers v and h. The probability that a certain state of v and h can be observed is given by the following joint distribution:\n",
    "<img src=\"figure4.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 2. Joint Distribution for v and h.\n",
    "Here Z is called the â€˜partition functionâ€™ that is the summation over all possible pairs of visible and hidden vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution is known as the Boltzmann Distribution which gives the probability that a particle can be observed in the state with the energy E. Unfortunately it is very difficult to calculate the joint probability due to the huge number of possible combination of v and h in the partition function Z. Much easier is the calculation of the conditional probabilities of state h given the state v and conditional probabilities of state v given the state h:\n",
    "<img src=\"figure5.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 3. Conditional probabilities for h and v.\n",
    "It should be noticed beforehand (before demonstrating this fact on practical example) that each neuron in a RBM can only exist in a binary state of 0 or 1. The most interesting factor is the probability that a hidden or visible layer neuron is in the state 1â€Šâ€”â€Šhence activated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Divergence\n",
    "\n",
    "### Gibbs Sampling\n",
    "The first part of the training is called Gibbs Sampling. Given an input vector v we are using p(h|v) for prediction of the hidden values h via sampling. Knowing the hidden values we use p(v|h) for prediction of new input values v via sampling. This process is repeated k times. After k iterations, we obtain the visible vector $v_k$ which was recreated from original input values $v_0$.\n",
    "<img src=\"figure8.png\" width=\"500\" height=\"300\" title=\"Layers in RBM\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gibbs function *gibbs* is divided into subparts: <br>\n",
    "1.*sampleHiddenLayer * <br>\n",
    "2.*sampleVisibleLayer*\n",
    "\n",
    "Let's look at *sampleHiddenLayer* now.\n",
    "\n",
    "### Sample Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know that given an input vector v the probability for a single hidden neuron j being activated is:\n",
    "<img src=\"figure6.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 4\n",
    "Here is Ïƒ the Sigmoid function.\n",
    "\n",
    "*sampleHiddenLayer* takes the visible layer as input to calculate the hidden layer using Eq. 4 *h1Pdf* and then samples it to get * h1_sample*\n",
    "\n",
    "    v_sample: given visible layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    hPdf: distribution of hidden layer; a matrix for batch of datapoints = p(h|v)\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleHiddenLayer(v_sample):\n",
    "    \n",
    "    # write the code for calculation of hPdf using vectorized implementation of Eq 4\n",
    "    hPdf = sigmoid(np.add(np.dot(v_sample,W),hiddenBias)) # ~ 1 line\n",
    "    \n",
    "    # Here, np.random.binomial is used to create the hidden layer sample matrix\n",
    "    h_sample = np_rng.binomial(size=hPdf.shape, n=1, p=hPdf)\n",
    "#     print(\"W[0]:\",W[0])\n",
    "#     print(\"hiddenBias:\",hiddenBias)\n",
    "#     print('h_sample.shape',h_sample.shape)\n",
    "#     print('v_sample.shape',v_sample.shape)\n",
    "    \n",
    "    return [hPdf, h_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Visible Layer\n",
    "Similarly, the probability that a binary state of a visible neuron i is set to 1 is:\n",
    "<img src=\"figure7.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 5\n",
    "\n",
    "As seen in equation 5, we will be writing a function to sample the Visible Layer.\n",
    "This function samples the visible layer based on the sampled data of hidden layer. <br>\n",
    "\n",
    "There are some differences in writing the function *sampleVisibleLayer*. <br>Firstly, we use np.random.multinomial to sample the visible layer *v_sample* from the distribution *vPdf*. <br>Secondly,elements of *vPdf* needs to sum to 1 as the function np.random.multinomial used to sample the visible layer takes on probability distributions as *pvals*. In other words, you are finding the softmax values. <br> Thirdly, we also make use of the *D* to sample the visible layer as each document has different word count.\n",
    "    \n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    vPdf: distribution of visible layer; a matrix for batch of datapoints = p(v|h)\n",
    "    v_sample: sampled visible layer matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleVisibleLayer(h_sample, D):\n",
    "    \n",
    "    # complete the following function such that vPdf has the sum of entries equal to 1 for each of the datapoints in the batch\n",
    "    # you have to use axis = 1 in writing the denominator\n",
    "    numerator = sigmoid(np.add(np.dot(h_sample,W.T),visibleBias))# ~1 line\n",
    "\n",
    "    denominator = numerator.sum(axis=1)# ~1 line\n",
    "    vPdf = (numerator/denominator[:,None]) #.flatten() # ~1 line\n",
    "    \n",
    "#     print('numerator.shape',numerator.shape)\n",
    "#     print('h_sample.shape',h_sample.shape)\n",
    "#     print('W.T.shape',W.T.shape)\n",
    "#     print('denominator.shape',denominator.shape)\n",
    "    \n",
    "    # Here np.random.multinomial is used to sample as each document has different number of words \n",
    "    # and hence D is also a parameter in sampling\n",
    "    v_sample = np.zeros((batchSize, vPdf.shape[1]))\n",
    "    for i in range(batchSize):\n",
    "        v_sample[i] = np_rng.multinomial(size=1, n=D[i], pvals=vPdf[i])\n",
    "    return [vPdf, v_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to write the function *gibbs* to run one iteration of gibbs sampling. Note that we are calculating the visible layer samples first and then using it to calculate he hidden layer sample. It'll become clear soon why we are doing so when you write the function for Contrastive Divergence.\n",
    "    \n",
    "    Input:\n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    Output:\n",
    "    vPdf: distribution of visible layer\n",
    "    v_sample: sampled visible layer matrix\n",
    "    hPdf: distribution of hidden layer\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def gibbs(h_sample, D):\n",
    "    \n",
    "    #use sampleVIsibleLayer and sampleHiddenLayer \n",
    "    vPdf, v_sample = sampleVisibleLayer(h_sample, D) # ~1 line\n",
    "    hPdf, h_sample = sampleHiddenLayer(v_sample) # ~1 line\n",
    "    return [vPdf, v_sample, hPdf, h_sample ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Contrastive Divergence\n",
    "\n",
    "You have learned that Contrastive Divergence updates weights after one iteration of Gibbs Sampling. Here, we shall perform *k* such iterations in Contrastive Divergence. \n",
    "The update of the weight matrix happens post the Contrastive Divergence step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will writing a funtion to run the contrastive divergence algorithm in k steps\n",
    "    \n",
    "    Input:\n",
    "    data: batch data (visible layer)\n",
    "    k: no of iterations for gibbs sampling\n",
    "    \n",
    "    Output:\n",
    "    hiddenPDF_data: distribution of the hidden layer based on data\n",
    "    visibleSamples: visible samples generated by gibbs sampling \n",
    "    hiddenPDF: distribution of the hidden layer based on samples generated by gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def cd_k(data,k):\n",
    "    \n",
    "    D = data.sum(axis=1)\n",
    "    hiddenPDF_data, hiddenSample_data = sampleHiddenLayer(data) # sample the hidden layer using the input data\n",
    "    chain_start = hiddenSample_data\n",
    "\n",
    "    for step in range(k):\n",
    "        if step == 0:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples  = gibbs(chain_start,D) # perform gibbs sampling using chain_start\n",
    "        else:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples = gibbs(hiddenSamples,D) # perform gibbs sampling using hiddenSamples\n",
    "    return hiddenPDF_data, visibleSamples, hiddenPDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Vectors $v_0$ and $v_k$ are used to calculate the activation probabilities for hidden layers $p(h_0|v_0)$ and $p(h_k|v_k)$ (Eq.4). The difference between the outer products of those probabilities with input vectors $v_0$ and $v_k$ results in the update matrix:\n",
    "<img src=\"figure9.png\" width=\"300\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 6. Update matrix. **Figure out** the vectorized implementation for this.\n",
    "\n",
    "In order to calculate $\\Delta (bias)$, <br>\n",
    "<center>$\\Delta (visiblebias) = average\\_ across\\_ batch(v_0 - v_k)$ </center> \n",
    "<center>$\\Delta (hiddenbias) = average\\_across\\_ batch(p(h_0|v_0) - p(h_k|v_k))$ </center> \n",
    "\n",
    "Using the update matrix the new weights can be calculated with momentum gradient ascent, given by:\n",
    "<center>  $mW_t = \\gamma \\ mW_{t-1} - \\Delta W$</center> \n",
    "<center>  $mvisiblebias_t = \\gamma \\ mvisiblebias_{t-1} - \\Delta visiblebias$</center>\n",
    "<center>  $mhiddenbias_t = \\gamma \\ mhiddenbias_{t-1} - \\Delta hiddenbias$</center><br>\n",
    "<center>  $W_t = W_{t-1} + \\alpha \\ mW_t$</center> \n",
    "<center>  $visiblebias_t = visiblebias_{t-1} + \\alpha \\ mvisiblebias_t$</center> \n",
    "<center>  $hiddenbias_t = hiddenbias_{t-1} + \\alpha \\ mhiddenbias_t$</center> \n",
    "\n",
    "\n",
    "Eq. 7. Update rule for the weights.\n",
    "\n",
    "Note that in the code implementation below <br>\n",
    " hiddenPDF_data is $p(h_0|v_0)$ <br>\n",
    " visibleSamples is $v_k$ <br>\n",
    " hiddenPDF is $p(h_k|v_k)$ <br>\n",
    " mdata is $v_0$ <br>\n",
    " eta is $\\alpha$ <br>\n",
    " mrate is $\\gamma$ <br>\n",
    "These will be helpful in writing the weight updates.\n",
    "\n",
    "In this we will write a function which iterates over our data for *epochs*.\n",
    "At every epoch we shuffle the data and then run CD on a mini batch size defined by *batchSize*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: I have experimented with momentum coeff, learning rate, epoch & no. of gibbs sampling interations. The results of these experiments is shown in the end of the notebook. **Please do not run the notebook at once as these may get over-written**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvisibleUnits: no of words in your Bag of words Model\\nhiddenUnits: no of topics\\nbatchSize: data slice to be selected \\nepochs: no of iterations\\neta: learning rate\\nmrate: momentum coefficient\\nW : weights between the visible and hidden layer\\nvisibleBias, hiddenBias: biases for visible and hidden layer respectively\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "visibleUnits: no of words in your Bag of words Model\n",
    "hiddenUnits: no of topics\n",
    "batchSize: data slice to be selected \n",
    "epochs: no of iterations\n",
    "eta: learning rate\n",
    "mrate: momentum coefficient\n",
    "W : weights between the visible and hidden layer\n",
    "visibleBias, hiddenBias: biases for visible and hidden layer respectively\n",
    "\"\"\"\n",
    "\n",
    "# define batch size\n",
    "batchSize = 200\n",
    "\n",
    "epochs = 10 \n",
    "eta = 0.001 \n",
    "mrate = 0.5\n",
    "\n",
    "# initialise weights\n",
    "weightinit=0.01\n",
    "W = weightinit * np_rng.randn(visibleUnits, hiddenUnits)\n",
    "visibleBias = weightinit * np_rng.randn(visibleUnits)\n",
    "hiddenBias = np.zeros((hiddenUnits))\n",
    "\n",
    "# number of interations of gibbs sampling\n",
    "k=500 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: I experimented with a few different ways to initialize momentum weights & biases and after some experimentation I went with the same initialization logic as the weight & bias for visible and hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def train(dataX,k):\n",
    "    \n",
    "    mW = 0.1 * np_rng.randn(visibleUnits, hiddenUnits)  # initialise momentum_weights\n",
    "    mvisibleBias =  np_rng.randn(visibleUnits)  # initialise momentum_visiblebiases\n",
    "    mhiddenBias =  np.zeros((hiddenUnits)) #hiddenBias # initialise momentum_hiddenbiases\n",
    "    global W,visibleBias,hiddenBias,mrate,batchSize,epochs # calling the variables initialized at the beginning\n",
    "#     print(\"initial hiddenBias:\", hiddenBias)\n",
    "#     print(\"initial hiddenBias[0]:\", W[0])\n",
    "    for epoch in range(epochs):\n",
    "        epoch_error.append(np.average(np.abs(eta * W)))\n",
    "        np_rng.shuffle(dataX) #shuffling the data\n",
    "        \n",
    "        for i in range(0, dataX.shape[0], batchSize):\n",
    "            mData = dataX[i:i+batchSize] #select a batch of datapoints\n",
    "            hiddenPDF_data, visibleSamples, hiddenPDF = cd_k(mData,k) # perfrom Contrastive Divergence on the batch for k iterations\n",
    "\n",
    "            mW = mrate*mW - (np.dot(mData.T,hiddenPDF_data) - np.dot(visibleSamples.T,hiddenPDF)) #write the momentum update equation for weight matrix\n",
    "            mvisibleBias = mrate*mvisibleBias - np.mean(mData-visibleSamples,axis=0) #write the momentum update equation for visiblebias vector\n",
    "            mhiddenBias = mrate*mhiddenBias - np.mean(hiddenPDF_data-hiddenPDF,axis=0) #write the momentum update equation for hiddenbias vector\n",
    "\n",
    "            W =  W + eta*mW#weight update equation\n",
    "            visibleBias =  visibleBias + eta*mvisibleBias #visible bias update equation\n",
    "            hiddenBias =  hiddenBias + eta*mhiddenBias #hidden bias update equation\n",
    "        \n",
    "#     print('W')\n",
    "#     print(W.T[0][:20])\n",
    "#     print(W.T[1][:20])\n",
    "#     print(W.T[4][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take around 10 minutes of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trainX.toarray(),k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Distribution based on Topics\n",
    "In this function, we are finding the distribution of words over the topics. You can take a look at the words under each topic and see what they are talking about. The number of topics is the number of neurons in the hidden layer. <br>\n",
    "<br>\n",
    "\n",
    "For each topic, the function prints the top 15 words that describe the topic. You can see that some of the words occur in multiple topics.\n",
    "\n",
    "    topic: number of hidden layers\n",
    "    voc: indexing of the vocabulary\n",
    "    \n",
    "Feel free to change the number of iterations of gibbs sampling in Contrastive Divergence and see how the distribution of words change under the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worddist( topic, voc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize every topic =1 once \n",
    "    \"\"\"\n",
    "    vecTopics = np.zeros((topic, topic))\n",
    "    for i in range(len(vecTopics)):\n",
    "        vecTopics[i][i] = 1\n",
    "    \n",
    "    \n",
    "    for i, vecTopic in enumerate(vecTopics):\n",
    "       \n",
    "        numerator = np.exp(np.dot(vecTopic, W.T) + visibleBias) #extract weights for current topic only and add bias\n",
    "        denominator = numerator.sum().reshape((1, 1))\n",
    "        word_distribution = (numerator/denominator).flatten() \n",
    "#         print('numerator.shape',numerator.shape)        \n",
    "#         print('word_distribution.shape',word_distribution.shape)\n",
    "        \n",
    "        tmpDict = {}\n",
    "        for j in voc.keys():\n",
    "            tmpDict[j] = word_distribution[voc[j]]\n",
    "            \n",
    "        print('topic', str(i), ':', vecTopic)\n",
    "        lm=0\n",
    "        for word, prob in sorted(tmpDict.items(), key=lambda x:x[1], reverse=True):\n",
    "            print ( word, str(prob))\n",
    "            lm+=1\n",
    "            if lm==15:\n",
    "                break\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: Following are the final outcomes from some (not all) experiments with the hyperparameters. The outcomes below are words with the highest probability (top 15) assigned to words for ever topic present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1** - Results with mrate=0.5,**eta=0.001,k=100**,epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "spinn 0.0006524152040901103\n",
      "controls 0.0006505715182136301\n",
      "hs850 0.0006481419325434056\n",
      "tone 0.0006447847169387012\n",
      "numbers 0.0006439053608644768\n",
      "regretted 0.0006436802948498797\n",
      "shiny 0.0006435412148758001\n",
      "speakerphone 0.0006431158154288662\n",
      "flaws 0.0006430379730379628\n",
      "drawback 0.0006424342772907553\n",
      "loose 0.0006422844509337879\n",
      "somewhat 0.0006419455676309631\n",
      "activesync 0.0006415275000868625\n",
      "dissapointed 0.0006410321691586049\n",
      "usage 0.000640975200333985\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "echo 0.0006543179137761169\n",
      "speakerphone 0.0006477231559574738\n",
      "controls 0.0006476400659267483\n",
      "lately 0.0006457033011311156\n",
      "cingulair 0.0006445616035771799\n",
      "music 0.0006440163053673764\n",
      "hs850 0.0006435972455827964\n",
      "shiny 0.0006435881870193016\n",
      "designed 0.0006434086709333896\n",
      "europe 0.0006420768262223621\n",
      "e715 0.0006417701890851023\n",
      "noted 0.0006415291059249597\n",
      "soon 0.0006415189969968092\n",
      "3o 0.0006410932027072436\n",
      "lesson 0.0006408920611860003\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "shiny 0.0006505131777529542\n",
      "hs850 0.0006447372865335025\n",
      "wood 0.0006443832758574243\n",
      "muddy 0.0006437316461869928\n",
      "controls 0.0006425412862308871\n",
      "3o 0.0006422734836356444\n",
      "sent 0.000642147202596026\n",
      "24 0.0006419804129363292\n",
      "wired 0.0006419663341890605\n",
      "adapter 0.0006409288373973651\n",
      "needless 0.0006408060315369288\n",
      "practically 0.0006401179010989977\n",
      "skip 0.0006395366588408587\n",
      "dissapointed 0.0006394384833506453\n",
      "accessing 0.0006388162027873867\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "im 0.0006500970110835307\n",
      "controls 0.000649285757087943\n",
      "supposedly 0.0006461667838683127\n",
      "spinn 0.0006460843168460817\n",
      "accessing 0.0006458498063358914\n",
      "procedure 0.0006441909712685813\n",
      "thorn 0.0006432891440812168\n",
      "shiny 0.0006431002974811815\n",
      "distorted 0.0006425335527164425\n",
      "died 0.0006411318181424728\n",
      "causing 0.0006409339030433922\n",
      "owned 0.0006404547320716755\n",
      "grtting 0.0006402741564453827\n",
      "regretted 0.0006400761174588092\n",
      "v265 0.0006398067841188955\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "controls 0.0006559207583341235\n",
      "24 0.0006469524302513101\n",
      "counter 0.0006462705166248343\n",
      "3o 0.0006454857003839587\n",
      "supposedly 0.0006433423218141942\n",
      "accessing 0.000642182807513727\n",
      "hs850 0.000641959735541607\n",
      "europe 0.0006417732416842681\n",
      "contract 0.0006411533345165012\n",
      "18 0.0006411407305216346\n",
      "dozens 0.0006410333416117717\n",
      "stop 0.000640304239546017\n",
      "sch 0.0006399563153376839\n",
      "research 0.0006394460556314618\n",
      "im 0.000638887771811879\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2** - Results with mrate=0.5,eta=0.001,**k=500**,epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "spinn 0.000649194443156827\n",
      "amazing 0.0006483275572622738\n",
      "regretted 0.0006441503106977802\n",
      "controls 0.0006433145775703704\n",
      "somewhat 0.0006424092617452834\n",
      "heavy 0.0006424032828502251\n",
      "gadgets 0.000642325677554717\n",
      "majority 0.0006413770161594453\n",
      "inform 0.0006412618917992938\n",
      "slider 0.0006411853041204485\n",
      "crappy 0.0006409221440915779\n",
      "loose 0.0006408289262711453\n",
      "exclaim 0.000640591031363337\n",
      "numbers 0.0006398689911235407\n",
      "specially 0.0006397041962328129\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "echo 0.0006495883679500362\n",
      "soon 0.0006459328032907043\n",
      "muddy 0.0006444324684799446\n",
      "e715 0.0006426542749020328\n",
      "deffinitely 0.0006426377509450384\n",
      "2mp 0.0006425245698774286\n",
      "speakerphone 0.0006420619777497197\n",
      "type 0.0006416737821420931\n",
      "3o 0.0006405973497929723\n",
      "friends 0.000640548152880026\n",
      "controls 0.0006404701344259259\n",
      "protective 0.0006398682801234707\n",
      "died 0.0006396240543064109\n",
      "regretted 0.000639539791951766\n",
      "noted 0.0006392708815084472\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "muddy 0.000652599876487995\n",
      "needless 0.0006479219041184884\n",
      "monkeys 0.0006458472484853681\n",
      "shiny 0.0006452334276175239\n",
      "according 0.0006437497552612867\n",
      "wood 0.0006430286843391764\n",
      "looses 0.0006421016373892633\n",
      "wired 0.000641956372123179\n",
      "kits 0.0006418754771768528\n",
      "3o 0.0006417836428410576\n",
      "changing 0.0006412755900087173\n",
      "latest 0.0006405776110345051\n",
      "skip 0.0006399841036912739\n",
      "gadgets 0.0006397811203445847\n",
      "sent 0.0006397381399350701\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "procedure 0.0006447093583391481\n",
      "needless 0.0006443994725876066\n",
      "kits 0.0006440924369076319\n",
      "amazing 0.000643438522605661\n",
      "im 0.0006431758105857962\n",
      "spinn 0.000642814435969865\n",
      "controls 0.0006421406073181013\n",
      "dirty 0.0006421259673049312\n",
      "owned 0.0006414681260054979\n",
      "bougth 0.0006412785736045703\n",
      "material 0.000641049786429011\n",
      "grtting 0.0006408521262427729\n",
      "v265 0.0006408507004324216\n",
      "died 0.0006407321117062381\n",
      "regretted 0.000640702330862437\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "controls 0.0006486666292586902\n",
      "deffinitely 0.0006464353798698642\n",
      "latest 0.000646318292370368\n",
      "3o 0.0006449844238788217\n",
      "friends 0.0006427342958076084\n",
      "counter 0.0006425335474528781\n",
      "shooters 0.0006415005748452088\n",
      "droid 0.0006414547927205196\n",
      "kits 0.0006406587113709732\n",
      "needless 0.0006404272827689539\n",
      "muddy 0.0006394404067591388\n",
      "owned 0.0006390096730995147\n",
      "monkeys 0.0006386825613307834\n",
      "managed 0.0006383309005528575\n",
      "24 0.0006381976844469\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 3** - Results with **mrate=0.8**,eta=0.001,k=500,epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "gadgets 0.0006688188940664466\n",
      "numbers 0.000664136210462444\n",
      "receive 0.0006629136023917666\n",
      "drawback 0.0006607478726347344\n",
      "overly 0.0006605251868973922\n",
      "steer 0.000659433033705906\n",
      "heavy 0.0006590764352005376\n",
      "hs850 0.0006589531213337341\n",
      "arguing 0.0006583260343221787\n",
      "enter 0.0006581631870000036\n",
      "slider 0.0006576393472864829\n",
      "contract 0.0006566068922049028\n",
      "monkeys 0.0006566022802014712\n",
      "flaws 0.0006565741862729577\n",
      "palmtop 0.0006558410236302972\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "receive 0.0006750350183489304\n",
      "lately 0.0006653190596512278\n",
      "overly 0.0006626604894263039\n",
      "muddy 0.0006606113941250434\n",
      "contract 0.0006600314182142065\n",
      "5020 0.0006596677823794536\n",
      "gadgets 0.0006596160503165852\n",
      "grtting 0.0006595543419812437\n",
      "colleague 0.0006587315200260943\n",
      "steer 0.0006582334302108911\n",
      "unacceptible 0.0006577013918603521\n",
      "e715 0.0006567518056067941\n",
      "mention 0.0006561431424203019\n",
      "protective 0.0006554691048581436\n",
      "designed 0.0006553676974423795\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "receive 0.0006731968734139401\n",
      "muddy 0.0006688265739977935\n",
      "monkeys 0.0006663057912608759\n",
      "gadgets 0.0006652379531624548\n",
      "hurt 0.0006620146576319161\n",
      "looses 0.0006618892993006205\n",
      "contract 0.0006618336786249082\n",
      "shiny 0.0006607393404775676\n",
      "numbers 0.0006584900030272231\n",
      "mother 0.0006584293297197832\n",
      "mentioned 0.0006576714712085374\n",
      "enter 0.0006569530341386856\n",
      "combination 0.0006564058794593683\n",
      "delivery 0.0006562123531017332\n",
      "ant 0.0006558441963112536\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "receive 0.0006696870578707487\n",
      "grtting 0.0006647098297458308\n",
      "overly 0.0006646468819113655\n",
      "mother 0.0006643921374456918\n",
      "steer 0.0006606320588427928\n",
      "pics 0.0006605251525925378\n",
      "colleague 0.0006603310689930127\n",
      "enter 0.0006601689884920588\n",
      "replaced 0.0006598511068582823\n",
      "thorn 0.0006595248759962052\n",
      "arrival 0.0006586919745578258\n",
      "accessing 0.0006581169126696934\n",
      "shooters 0.0006579669260822195\n",
      "iriver 0.0006572413104821022\n",
      "puff 0.000655867087802515\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "enter 0.0006655955294848297\n",
      "overly 0.0006649579079551179\n",
      "shooters 0.000664818070063113\n",
      "contract 0.0006644008746336702\n",
      "receive 0.0006637712499596099\n",
      "v3c 0.0006605090535530816\n",
      "monkeys 0.0006590374262279917\n",
      "steer 0.0006589681967782336\n",
      "5020 0.0006576202527166279\n",
      "gadgets 0.0006570752693961735\n",
      "looses 0.0006570269909984554\n",
      "unacceptible 0.000656868415089189\n",
      "18 0.0006568270342861729\n",
      "lately 0.0006567180452985963\n",
      "colleague 0.0006565920321032219\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4** - Results with mrate=0.5,eta=0.001,**k=1000**,epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "spinn 0.000648626254842024\n",
      "pain 0.0006449641568013654\n",
      "loose 0.0006448808319196189\n",
      "controls 0.0006438788868137451\n",
      "regretted 0.000642668508680707\n",
      "somewhat 0.0006423241281897415\n",
      "slider 0.0006421113768316097\n",
      "complaints 0.0006409632747041718\n",
      "dissapointed 0.0006404544331099966\n",
      "speakerphone 0.000640215154251622\n",
      "scratched 0.000640188797354127\n",
      "arguing 0.0006401763066359821\n",
      "haven 0.0006399249148608437\n",
      "hs850 0.0006396813236744327\n",
      "severe 0.0006395825325553146\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "speakerphone 0.0006449305698656856\n",
      "cingulair 0.000643973263473468\n",
      "type 0.0006437066564077254\n",
      "pain 0.0006434861904794708\n",
      "protective 0.0006431809723847077\n",
      "echo 0.0006410558955848775\n",
      "controls 0.000640993752613397\n",
      "3o 0.0006409175138877158\n",
      "generally 0.0006407268419302515\n",
      "noted 0.0006407050199849747\n",
      "sources 0.0006399400467966577\n",
      "e715 0.0006396823777407392\n",
      "wood 0.0006394703022148695\n",
      "2mp 0.0006392565212680539\n",
      "overly 0.0006390813463366422\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "wood 0.0006440391304989238\n",
      "needless 0.000643333817423774\n",
      "muddy 0.000642494002553562\n",
      "sent 0.0006423974912454512\n",
      "looses 0.0006421421985505829\n",
      "3o 0.0006420994362215693\n",
      "options 0.0006420741841148363\n",
      "rotating 0.0006410767388817471\n",
      "cheaply 0.0006402676671352832\n",
      "monkeys 0.0006398940321971794\n",
      "practically 0.0006392768424938872\n",
      "renders 0.0006392408428946749\n",
      "metal 0.0006391968297272343\n",
      "shiny 0.0006391060297328059\n",
      "type 0.0006389292939938273\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "pain 0.0006481030058189717\n",
      "kindle 0.0006447696336006021\n",
      "causing 0.0006432530996305759\n",
      "controls 0.0006425988461950932\n",
      "spinn 0.0006423887009248827\n",
      "procedure 0.0006415250543543666\n",
      "bougth 0.0006413455294788579\n",
      "receiving 0.0006412413167797692\n",
      "arrival 0.0006412344766419735\n",
      "env 0.0006410092108637199\n",
      "owned 0.0006409093766332051\n",
      "overly 0.0006408746737233879\n",
      "pics 0.0006400107126432338\n",
      "needless 0.0006399276532832986\n",
      "grtting 0.0006395535656106466\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "controls 0.0006491411908572735\n",
      "3o 0.0006453405045206912\n",
      "latest 0.0006429612944975051\n",
      "cheaply 0.0006414328330967428\n",
      "overly 0.0006413659693871289\n",
      "shooters 0.0006413269085953618\n",
      "18 0.0006406156172618818\n",
      "texas 0.000640565345024625\n",
      "pain 0.0006404634633382622\n",
      "scratched 0.0006391375985112457\n",
      "intended 0.0006390734422628218\n",
      "ill 0.0006388922521340539\n",
      "loose 0.0006388788540989626\n",
      "type 0.0006387853088660145\n",
      "reccomendation 0.0006386604905258066\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: All experiments that I ran are not shown above, this is only a subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: Experiment 3 far provided the best results in terms of placing similar words in the same topic. For example, topic 1 includes words like 'regretted', 'complaints', 'disappointed' associated to negative reviews & complaints; topic 2 includes words describing products such as 'speakerphone', 'echo', 'controls' and so on. \n",
    "\n",
    "\n",
    ">**Conclusion**: With the default parameters i.e. with low K, high epochs and relatively high learning rate, the top 15 words across  the top 5 words were mostly the same. However, increasing  no. of iterations of gibbs sampling (k) and even with few epochs and lower learning rate we get more optimal results where we can see different set of top 15 words in the 5 topics. The model can be improved further by tweaking the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
